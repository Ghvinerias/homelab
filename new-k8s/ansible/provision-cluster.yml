- name: Provision Kubernetes control plane (simple)
  hosts: k8s_cluster
  become: true
  gather_facts: false

  vars:
    KUBE_VERSION: "v1.31.0"
    CONTAINERD_VERSION: "1.7.22"
    RUNC_VERSION: "1.2.1"
    CNI_PLUGINS_VERSION: "1.5.1"
    POD_NETWORK_CIDR: "10.244.0.0/16"
    MASTER_PRIVATE_IP: "{{ master_private_ip | default('10.11.99.61') }}"
    EXTRA_API_SERVER_SANS: "{{ '127.0.0.1,10.11.99.50,k8s-mgmt.infra.slick.ge' }}"
    CONTROL_PLANE_ENDPOINT: "10.11.99.50:6443"

  environment:
    DEBIAN_FRONTEND: noninteractive

  tasks:
    - name: Wait for SSH to be ready (control plane & nodes)
      ansible.builtin.wait_for_connection:
        delay: 5
        sleep: 10
        timeout: 600

    - name: Kernel modules file
      ansible.builtin.copy:
        dest: /etc/modules-load.d/k8s.conf
        mode: "0644"
        content: |
          overlay
          br_netfilter

    - name: Load kernel modules
      ansible.builtin.shell: |
        modprobe overlay
        modprobe br_netfilter

    - name: Sysctl config
      ansible.builtin.copy:
        dest: /etc/sysctl.d/k8s.conf
        mode: "0644"
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
          net.ipv6.conf.default.forwarding    = 1

    - name: Apply sysctl
      ansible.builtin.shell: sysctl --system

    - name: Download containerd
      ansible.builtin.get_url:
        url: "https://github.com/containerd/containerd/releases/download/v{{ CONTAINERD_VERSION }}/containerd-{{ CONTAINERD_VERSION }}-linux-amd64.tar.gz"
        dest: /tmp/containerd.tgz
        mode: "0644"

    - name: Install containerd
      ansible.builtin.unarchive:
        src: /tmp/containerd.tgz
        dest: /usr/local
        remote_src: true

    - name: Install containerd systemd service
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
        dest: /usr/lib/systemd/system/containerd.service
        mode: "0644"

    - name: Install runc
      ansible.builtin.get_url:
        url: "https://github.com/opencontainers/runc/releases/download/v{{ RUNC_VERSION }}/runc.amd64"
        dest: /usr/local/sbin/runc
        mode: "0755"

    - name: Ensure CNI dir
      ansible.builtin.file:
        path: /opt/cni/bin
        state: directory
        mode: "0755"

    - name: Download CNI plugins
      ansible.builtin.get_url:
        url: "https://github.com/containernetworking/plugins/releases/download/v{{ CNI_PLUGINS_VERSION }}/cni-plugins-linux-amd64-v{{ CNI_PLUGINS_VERSION }}.tgz"
        dest: /tmp/cni.tgz
        mode: "0644"

    - name: Install CNI plugins
      ansible.builtin.unarchive:
        src: /tmp/cni.tgz
        dest: /opt/cni/bin
        remote_src: true

    - name: Ensure containerd dir
      ansible.builtin.file:
        path: /etc/containerd/
        state: directory
        mode: "0755"

    - name: Generate containerd config
      ansible.builtin.shell: "containerd config default | tee /etc/containerd/config.toml >/dev/null"
      args:
        creates: /etc/containerd/config.toml

    - name: Enable systemd cgroup in containerd
      ansible.builtin.replace:
        path: /etc/containerd/config.toml
        regexp: "SystemdCgroup = false"
        replace: "SystemdCgroup = true"

    - name: Enable and start containerd
      ansible.builtin.systemd:
        name: containerd
        enabled: true
        state: started
        daemon_reload: true

    - name: Base deps
      ansible.builtin.apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gpg
        state: present
        update_cache: true

    - name: Ensure apt keyrings dir
      ansible.builtin.file:
        path: /etc/apt/keyrings
        state: directory
        mode: "0755"

    - name: Add Kubernetes apt key
      ansible.builtin.shell: |
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | \
        gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Add Kubernetes apt repo
      ansible.builtin.copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        mode: "0644"
        content: |
          deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /

    - name: Install kubelet/kubeadm/kubectl and other tools
      ansible.builtin.apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
          - ansible
          - git
          - curl
          - net-tools
          - gnupg2
          - software-properties-common
          - apt-transport-https
          - ca-certificates
          - wget
          - jq
          - socat
          - conntrack
          - ipset
        state: present
        update_cache: true

    - name: Hold Kubernetes packages
      ansible.builtin.shell: apt-mark hold kubelet kubeadm kubectl

    - name: Pull control plane images
      ansible.builtin.command: kubeadm config images pull
      register: pull_images
      failed_when: pull_images.rc != 0
      run_once: true

    - name: Pull kube-vip image
      ansible.builtin.shell: |
        export KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r ".[0].name")
        ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Generate kube-vip manifest on first control plane
      ansible.builtin.shell: |
        export VIP=10.11.99.50
        export INTERFACE=eth0
        export KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r ".[0].name")
        ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip manifest pod \
          --interface $INTERFACE \
          --address $VIP \
          --controlplane \
          --services \
          --arp \
          --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Update kube-vip to use super-admin.conf for bootstrap
      ansible.builtin.replace:
        path: /etc/kubernetes/manifests/kube-vip.yaml
        regexp: 'path: /etc/kubernetes/admin.conf'
        replace: 'path: /etc/kubernetes/super-admin.conf'
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: kubeadm init
      ansible.builtin.command: >
        kubeadm init
        --pod-network-cidr={{ POD_NETWORK_CIDR }}
        --kubernetes-version={{ KUBE_VERSION }}
        --apiserver-advertise-address={{ MASTER_PRIVATE_IP }}
        --apiserver-cert-extra-sans={{ EXTRA_API_SERVER_SANS }}
        --control-plane-endpoint={{ CONTROL_PLANE_ENDPOINT }}
        --ignore-preflight-errors=NumCPU
        --upload-certs
      args:
        creates: /etc/kubernetes/admin.conf
      register: kubeadm_init
      failed_when: kubeadm_init.rc != 0 and kubeadm_init.rc != 1
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Verify kubeadm init success
      ansible.builtin.stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf
      failed_when: not admin_conf.stat.exists
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Configure kubectl for root
      ansible.builtin.shell: |
        mkdir -p /root/.kube
        cp -i /etc/kubernetes/admin.conf /root/.kube/config
        chown root:root /root/.kube/config
      args:
        creates: /root/.kube/config
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Update kube-vip to use admin.conf after bootstrap
      ansible.builtin.replace:
        path: /etc/kubernetes/manifests/kube-vip.yaml
        regexp: 'path: /etc/kubernetes/super-admin.conf'
        replace: 'path: /etc/kubernetes/admin.conf'
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Wait for kube-vip to be ready
      ansible.builtin.wait_for:
        host: "10.11.99.50"
        port: 6443
        timeout: 300
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Verify cluster is accessible
      ansible.builtin.command: kubectl --kubeconfig=/etc/kubernetes/super-admin.conf get nodes
      register: cluster_status
      retries: 10
      delay: 15
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Deploy Flannel
      ansible.builtin.command: >
        kubectl --kubeconfig=/etc/kubernetes/super-admin.conf apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml --validate=false
      register: flannel_deploy
      failed_when: flannel_deploy.rc != 0
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Wait for Flannel pods to be ready
      ansible.builtin.command: kubectl --kubeconfig=/etc/kubernetes/super-admin.conf get pods -n kube-flannel -o jsonpath='{.items[*].status.phase}'
      register: flannel_status
      until: "'Pending' not in flannel_status.stdout and 'ContainerCreating' not in flannel_status.stdout"
      retries: 60
      delay: 10
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"
    
    - name: Save and show join command
      ansible.builtin.shell: "KUBECONFIG=/etc/kubernetes/super-admin.conf kubeadm token create --print-join-command | tee /root/join.sh"
      register: join_cmd
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Join command (debug)
      ansible.builtin.debug:
        var: join_cmd.stdout
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Read kubeconfig from control plane
      ansible.builtin.slurp:
        src: /root/.kube/config
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"
      register: kubeconfig_b64

    - name: Write kubeconfig to local file
      become: false
      ansible.builtin.copy:
        dest: ./kube_config
        mode: "0600"
        content: "{{ kubeconfig_b64.content | b64decode }}"
      run_once: true
      delegate_to: localhost

    - name: Ensure local kube dir exists (~/.kube)
      become: false
      ansible.builtin.file:
        path: "{{ lookup('env','HOME') }}/.kube"
        state: directory
        mode: "0700"
      run_once: true
      delegate_to: localhost
      when: setup_local_kubeconfig | default(false) | bool

    - name: Copy local kube_config to ~/.kube/config
      become: false
      ansible.builtin.copy:
        src: ./kube_config
        dest: "{{ lookup('env','HOME') }}/.kube/config"
        mode: "0600"
      run_once: true
      delegate_to: localhost
      when: setup_local_kubeconfig | default(false) | bool
       
# Join additional control-plane nodes
- name: Join additional control-plane nodes to the cluster
  hosts: control_plane
  become: true
  gather_facts: false
  tasks:
    - name: Wait for SSH to be ready (control-plane join)
      ansible.builtin.wait_for_connection:
        delay: 5
        sleep: 10
        timeout: 600
      when: inventory_hostname != groups['control_plane'][0]

    - name: Get kubeadm join command with cert key
      ansible.builtin.shell: |
        KUBECONFIG=/etc/kubernetes/super-admin.conf kubeadm token create --print-join-command --certificate-key $(KUBECONFIG=/etc/kubernetes/super-admin.conf kubeadm init phase upload-certs --upload-certs | tail -1)
      register: join_cp_cmd_output
      run_once: true
      delegate_to: "{{ groups['control_plane'][0] }}"
    - name: Set join command fact for control-plane
      ansible.builtin.set_fact:
        join_cp_cmd: "{{ join_cp_cmd_output.stdout | trim }} --control-plane"
      when: inventory_hostname != groups['control_plane'][0]

    - name: Write join script on target (control-plane)
      ansible.builtin.copy:
        dest: /root/join-control-plane.sh
        mode: "0755"
        content: "{{ join_cp_cmd }}"
      when: inventory_hostname != groups['control_plane'][0]

    - name: Pull kube-vip image on additional control plane nodes
      ansible.builtin.shell: |
        export KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r ".[0].name")
        ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION
      when: inventory_hostname != groups['control_plane'][0]

    - name: Generate kube-vip manifest on additional control plane nodes
      ansible.builtin.shell: |
        export VIP=10.11.99.50
        export INTERFACE=eth0
        export KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r ".[0].name")
        ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip manifest pod \
          --interface $INTERFACE \
          --address $VIP \
          --controlplane \
          --services \
          --arp \
          --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml
      when: inventory_hostname != groups['control_plane'][0]

    - name: Wait for VIP to be accessible from joining nodes
      ansible.builtin.wait_for:
        host: "10.11.99.50"
        port: 6443
        timeout: 120
      when: inventory_hostname != groups['control_plane'][0]

    - name: Test API server connectivity before join
      ansible.builtin.command: curl -k https://10.11.99.50:6443/healthz
      register: api_health
      when: inventory_hostname != groups['control_plane'][0]
      ignore_errors: true

    - name: Execute join (idempotent, control-plane)
      ansible.builtin.command: "{{ join_cp_cmd }}"
      args:
        creates: /etc/kubernetes/kubelet.conf
      register: node_join_cp
      failed_when: node_join_cp.rc != 0 and 'already exists' not in node_join_cp.stderr
      retries: 3
      delay: 30
      until: node_join_cp.rc == 0 or 'already exists' in node_join_cp.stderr
      when: inventory_hostname != groups['control_plane'][0]

    - name: Verify node joined successfully (control-plane)
      ansible.builtin.stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf_cp
      failed_when: not kubelet_conf_cp.stat.exists
      when: inventory_hostname != groups['control_plane'][0]

    - name: Wait for node to be ready (control-plane)
      ansible.builtin.command: kubectl get node {{ inventory_hostname }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
      register: node_ready_cp
      until: node_ready_cp.stdout == "True"
      retries: 20
      delay: 15
      delegate_to: "{{ groups['control_plane'][0] }}"
      when: inventory_hostname != groups['control_plane'][0]

# Remove control-plane taint and exclude-from-external-load-balancers label from all nodes
- name: Remove control-plane taint and exclude-from-external-load-balancers label from all nodes
  hosts: k8s_cluster
  become: true
  gather_facts: false
  tasks:
    - name: Remove control-plane taint (NoSchedule) from node
      ansible.builtin.command: >
        kubectl taint nodes {{ inventory_hostname }} node-role.kubernetes.io/control-plane:NoSchedule-
      register: taint_remove
      failed_when: taint_remove.rc != 0 and 'not found' not in taint_remove.stderr and 'taint key not found' not in taint_remove.stderr
      delegate_to: "{{ groups['control_plane'][0] }}"

    - name: Remove exclude-from-external-load-balancers label from node
      ansible.builtin.command: >
        kubectl label node {{ inventory_hostname }} node.kubernetes.io/exclude-from-external-load-balancers-
      register: label_remove
      failed_when: label_remove.rc != 0 and 'not found' not in label_remove.stderr and 'label not found' not in label_remove.stderr
      delegate_to: "{{ groups['control_plane'][0] }}"
