---
- name: Deploy Grafana and Prometheus Stack with Helm
  hosts: localhost
  gather_facts: false
  connection: local
  become: false
  vars:
    kubeconfig: "{{ lookup('env', 'KUBECONFIG') | default('~/.kube/config', true) }}"
    kube_context: "" # Set to your context if needed, else leave blank
    deployment_namespace: "monitoring"
    infisical_host_api: "https://infisical.infra.slick.ge/api"
    identity_id: "998c1306-87ff-4b1e-9225-2f82ef8d5fff"
    project_slug: "infra-resources-b-ou-q"
    env_slug: "homelablocal"
    secrets_path: "/grafana"
    grafana_ingress_host: "grafana.infra.k8s.slick.ge"
    prometheus_ingress_host: "prometheus.infra.k8s.slick.ge"
    alertmanager_ingress_host: "alertmanager.infra.k8s.slick.ge"
    otel_collector_ingress_host: "otel.infra.k8s.slick.ge"

    # Optional: Enable and define external node_exporter hosts
    enable_external_nodeexporter: true
    external_nodeexporter_targets:
      - "10.10.10.13:9400"
      - "10.10.10.13:9100"
      - "10.10.10.13:9835"

  tasks:
    # Inline additionalScrapeConfigs for external node_exporter hosts
    - name: Set additional scrape configs for external node_exporters
      set_fact:
        external_additional_scrape_configs:
          - job_name: 'external-node-exporters'
            scheme: http
            metrics_path: /metrics
            static_configs:
              - targets: "{{ external_nodeexporter_targets }}"
      when: enable_external_nodeexporter
    - name: Create monitoring namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ deployment_namespace }}"
        kubeconfig: "{{ kubeconfig }}"
        context: "{{ kube_context if kube_context else omit }}"

    - name: Create InfisicalSecret for Grafana admin credentials
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: secrets.infisical.com/v1alpha1
          kind: InfisicalSecret
          metadata:
            name: grafana-admin
            namespace: "{{ deployment_namespace }}"
          spec:
            hostAPI: "{{ infisical_host_api }}"
            resyncInterval: 10
            authentication:
              kubernetesAuth:
                identityId: "{{ identity_id }}"
                serviceAccountRef:
                  name: infisical-service-account
                  namespace: infisical
                secretsScope:
                  envSlug: "{{ env_slug }}"
                  projectSlug: "{{ project_slug }}"
                  secretsPath: "{{ secrets_path }}"
            managedKubeSecretReferences:
              - secretName: grafana-admin
                secretNamespace: "{{ deployment_namespace }}"
                creationPolicy: Orphan
                template:
                  includeAllSecrets: false
                  data:
                    admin-user: "{{ '{{' }} .GRAFANA_ADMIN_USER.Value {{ '}}' }}"
                    admin-password: "{{ '{{' }} .GRAFANA_ADMIN_PASSWORD.Value {{ '}}' }}"
        kubeconfig: "{{ kubeconfig }}"
        context: "{{ kube_context if kube_context else omit }}"

    - name: Wait for grafana-admin Secret to be created by Infisical
      kubernetes.core.k8s_info:
        kind: Secret
        name: grafana-admin
        namespace: "{{ deployment_namespace }}"
        kubeconfig: "{{ kubeconfig }}"
        context: "{{ kube_context if kube_context else omit }}"
      register: grafana_secret
      until: grafana_secret.resources | length > 0
      retries: 30
      delay: 10
      failed_when: grafana_secret.resources | length == 0

    - name: Ensure prometheus-community Helm repo is present
      kubernetes.core.helm_repository:
        name: prometheus-community
        repo_url: https://prometheus-community.github.io/helm-charts

    - name: Ensure open-telemetry Helm repo is present
      kubernetes.core.helm_repository:
        name: open-telemetry
        repo_url: https://open-telemetry.github.io/opentelemetry-helm-charts

    - name: Deploy kube-prometheus-stack via Helm
      kubernetes.core.helm:
        name: kube-prometheus-stack
        chart_ref: prometheus-community/kube-prometheus-stack
        timeout: 10m
        release_namespace: "{{ deployment_namespace }}"
        create_namespace: true
        values:
          # kube-prometheus-stack values override for homelab
          # Chart: prometheus-community/kube-prometheus-stack
          nameOverride: ""
          fullnameOverride: ""
          grafana:
            enabled: true
            defaultDashboardsEnabled: true
            # Grafana plugins to install on startup
            plugins:
              - grafana-clock-panel
              - grafana-simple-json-datasource
              - grafana-piechart-panel
              - alexanderzobnin-zabbix-app
              - fetzerch-sunandmoon-datasource
              # Add more plugins as needed
            # Use an existing secret for admin credentials to avoid committing secrets
            admin:
              existingSecret: grafana-admin
              userKey: admin-user
              passwordKey: admin-password
            service:
              type: ClusterIP
              port: 80
            ingress:
              enabled: true
              ingressClassName: nginx
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt-prod"
                kubernetes.io/tls-acme: "true"
              hosts:
                - "{{ grafana_ingress_host }}"
              path: /
              pathType: Prefix
              tls:
                - secretName: grafana-tls
                  hosts:
                    - "{{ grafana_ingress_host }}"
            persistence:
              enabled: true
              type: pvc
              accessModes:
                - ReadWriteOnce
              size: 10Gi
              storageClassName: "longhorn"
          prometheus:
            enabled: true
            service:
              type: ClusterIP
              port: 9090
            ingress:
              enabled: true
              ingressClassName: nginx
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt-prod"
                kubernetes.io/tls-acme: "true"
              hosts:
                - "{{ prometheus_ingress_host }}"
              paths:
                - /
              pathType: Prefix
              tls:
                - secretName: prometheus-tls
                  hosts:
                    - "{{ prometheus_ingress_host }}"
            prometheusSpec:
              retention: 15d
              # Enable remote write receiver for OTEL
              enableRemoteWriteReceiver: true
              # Adjust resources for homelab-scale
              resources:
                requests:
                  cpu: 250m
                  memory: 512Mi
                limits:
                  cpu: 1
                  memory: 2Gi
              storageSpec:
                volumeClaimTemplate:
                  spec:
                    storageClassName: "longhorn"
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: 4Gi
              # Scrape your own ServiceMonitors/PodMonitors without extra labels
              serviceMonitorSelector: {}
              serviceMonitorNamespaceSelector: {}
              podMonitorSelector: {}
              podMonitorNamespaceSelector: {}
              # Optionally add additional scrape configs for external node_exporters
              additionalScrapeConfigs: "{{ external_additional_scrape_configs | default([]) | to_nice_yaml }}"
          alertmanager:
            enabled: true
            service:
              type: ClusterIP
              port: 9093
            ingress:
              enabled: true
              ingressClassName: nginx
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt-prod"
                kubernetes.io/tls-acme: "true"
              hosts:
                - "{{ alertmanager_ingress_host }}"
              path: /
              pathType: Prefix
              tls:
                - secretName: alertmanager-tls
                  hosts:
                    - "{{ alertmanager_ingress_host }}"
            alertmanagerSpec:
              replicas: 1
              resources:
                requests:
                  cpu: 100m
                  memory: 256Mi
                limits:
                  cpu: 500m
                  memory: 512Mi
              storage:
                volumeClaimTemplate:
                  spec:
                    storageClassName: "longhorn"
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: 5Gi
          prometheusOperator:
            enabled: true
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 500m
                memory: 512Mi
          kubeProxy:
            enabled: false
          kubeEtcd:
            enabled: true
          kube-state-metrics:
            enabled: true
          nodeExporter:
            enabled: true
          ## Optional: enable ingress for thanos-query or other components later
          ## thanosRuler:
          ##   enabled: false

        kubeconfig: "{{ kubeconfig }}"
        context: "{{ kube_context if kube_context else omit }}"
        wait: true
        atomic: true
        update_repo_cache: true

    - name: Deploy OpenTelemetry Collector via Helm
      kubernetes.core.helm:
        name: opentelemetry-collector
        chart_ref: open-telemetry/opentelemetry-collector
        release_namespace: "{{ deployment_namespace }}"
        create_namespace: true
        values:
          mode: daemonset
          
          # Host network for direct access on node IPs
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          
          # Image configuration (required)
          image:
            repository: otel/opentelemetry-collector-contrib
            tag: 0.115.1
          
          # Allow scheduling on control-plane nodes
          tolerations:
            - key: "node-role.kubernetes.io/control-plane"
              operator: "Exists"
              effect: "NoSchedule"
            - key: "node-role.kubernetes.io/master"
              operator: "Exists"
              effect: "NoSchedule"
          
          # Resource limits for homelab
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          
          # Service configuration - LoadBalancer to expose externally
          service:
            type: LoadBalancer
          
          # Ports for OTLP receivers
          ports:
            otlp:
              enabled: true
              containerPort: 4317
              servicePort: 4317
              hostPort: 4317
              protocol: TCP
            otlp-http:
              enabled: true
              containerPort: 4318
              servicePort: 4318
              hostPort: 4318
              protocol: TCP
            metrics:
              enabled: true
              containerPort: 8888
              servicePort: 8888
              protocol: TCP
          
          # OpenTelemetry Collector configuration
          config:
            receivers:
              otlp:
                protocols:
                  grpc:
                    endpoint: 0.0.0.0:4317
                  http:
                    endpoint: 0.0.0.0:4318
            
            processors:
              batch:
                timeout: 10s
                send_batch_size: 1024
              memory_limiter:
                check_interval: 5s
                limit_mib: 400
            
            exporters:
              # Export metrics to Prometheus
              prometheus:
                endpoint: 0.0.0.0:8889
                namespace: otel
              
              # Export metrics via remote write to Prometheus
              prometheusremotewrite:
                endpoint: http://kube-prometheus-stack-prometheus.{{ deployment_namespace }}.svc.cluster.local:9090/api/v1/write
                tls:
                  insecure: true
              
              # Export logs to Loki
              loki:
                endpoint: http://loki-gateway.logging.svc.cluster.local/loki/api/v1/push
                tls:
                  insecure: true
              
              # Debug exporter for troubleshooting
              debug:
                verbosity: normal
            
            service:
              pipelines:
                traces:
                  receivers: [otlp]
                  processors: [memory_limiter, batch]
                  exporters: [debug]
                metrics:
                  receivers: [otlp]
                  processors: [memory_limiter, batch]
                  exporters: [prometheus, prometheusremotewrite]
                logs:
                  receivers: [otlp]
                  processors: [memory_limiter, batch]
                  exporters: [loki, debug]
          
          # ServiceMonitor for Prometheus to scrape otel collector metrics
          serviceMonitor:
            enabled: true
            additionalLabels:
              release: kube-prometheus-stack
        
        kubeconfig: "{{ kubeconfig }}"
        context: "{{ kube_context if kube_context else omit }}"
        wait: true
